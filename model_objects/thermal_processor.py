import os
import cv2
import numpy as np
from pathlib import Path
from ultralytics import YOLO
from typing import List, Tuple, Dict

class ThermalProcessor:
    def __init__(self):
        # –ü—É—Ç–∏ –∏ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏
        self.model_path = str(Path(__file__).parent / "best_thermal.pt")
        self.MIN_CONFIDENCE = 0.4  # –ü–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ (50%)
        self.MIN_DETECTION_TIME = 1.00  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è –¥–µ—Ç–µ–∫—Ü–∏–∏ (—Å–µ–∫)
        
        # –ö–ª–∞—Å—Å—ã –æ–±—ä–µ–∫—Ç–æ–≤   
        self.class_names = {
            0: 'car',
            1: 'animals', 
            2: 'people'
        }
        
        # –¶–≤–µ—Ç–∞ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤ (BGR)
        self.class_colors = {
            0: (0, 255, 0),    # –ß–µ–ª–æ–≤–µ–∫ - –∑–µ–ª–µ–Ω—ã–π
            1: (0, 0, 255),    # –ñ–∏–≤–æ—Ç–Ω–æ–µ - –∫—Ä–∞—Å–Ω—ã–π
            2: (255, 0, 0)     # –ú–∞—à–∏–Ω–∞ - —Å–∏–Ω–∏–π
        }
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
        self._verify_model_file()
        self.model = YOLO(self.model_path)
    
    def _verify_model_file(self):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ —Ñ–∞–π–ª–∞ –º–æ–¥–µ–ª–∏"""
        if not os.path.exists(self.model_path):
            available_files = "\n".join(os.listdir(Path(__file__).parent))
            raise FileNotFoundError(
                f"–§–∞–π–ª –º–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω –ø–æ –ø—É—Ç–∏: {self.model_path}\n"
                f"–î–æ—Å—Ç—É–ø–Ω—ã–µ —Ñ–∞–π–ª—ã:\n{available_files}"
            )

    async def process_video(self, video_path: str) -> List[List[Tuple[int, float, Tuple[int, int, int, int]]]]:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤–∏–¥–µ–æ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –¥–µ—Ç–µ–∫—Ü–∏–∏ –ø–æ –∫–∞–¥—Ä–∞–º"""
        cap = cv2.VideoCapture(video_path)
        detections = []
        
        while True:
            ret, frame = cap.read()
            if not ret:
                break
                
            # –î–µ—Ç–µ–∫—Ü–∏—è –æ–±—ä–µ–∫—Ç–æ–≤
            results = self.model(frame, verbose=False)
            frame_dets = []
            
            for box in results[0].boxes:
                if box.conf.item() < self.MIN_CONFIDENCE:
                    continue
                    
                cls_id = int(box.cls.item())
                bbox = [
                    int(box.xyxy[0][0].item()),
                    int(box.xyxy[0][1].item()),
                    int(box.xyxy[0][2].item()),
                    int(box.xyxy[0][3].item())
                ]
                frame_dets.append((cls_id, box.conf.item(), bbox))
            
            detections.append(frame_dets)
        
        cap.release()
        return detections
    
    async def process_and_visualize_video(self, input_path: str, output_path: str) -> str:
        """
        –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤–∏–¥–µ–æ, —Ä–∏—Å—É–µ—Ç bounding boxes –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—É—Ç—å –∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–º—É –≤–∏–¥–µ–æ
        """
        cap = cv2.VideoCapture(input_path)
        if not cap.isOpened():
            raise ValueError(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–∫—Ä—ã—Ç—å –≤–∏–¥–µ–æ—Ñ–∞–π–ª: {input_path}")
        
        # –ü–æ–ª—É—á–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤–∏–¥–µ–æ
        fps = cap.get(cv2.CAP_PROP_FPS)
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        
        # –°–æ–∑–¥–∞–µ–º VideoWriter –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
        
        frame_count = 0
        while True:
            ret, frame = cap.read()
            if not ret:
                break
                
            # –î–µ—Ç–µ–∫—Ü–∏—è –æ–±—ä–µ–∫—Ç–æ–≤
            results = self.model(frame, verbose=False)
            
            # –û—Ç—Ä–∏—Å–æ–≤–∫–∞ bounding boxes
            for box in results[0].boxes:
                if box.conf.item() < self.MIN_CONFIDENCE:
                    continue
                    
                cls_id = int(box.cls.item())
                bbox = box.xyxy[0].cpu().numpy().astype(int)
                color = self.class_colors.get(cls_id, (255, 255, 255))
                
                # –†–∏—Å—É–µ–º –ø—Ä—è–º–æ—É–≥–æ–ª—å–Ω–∏–∫
                cv2.rectangle(frame, (bbox[0], bbox[1]), (bbox[2], bbox[3]), color, 2)
                
                # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—Å—Ç —Å –∫–ª–∞—Å—Å–æ–º –∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é
                label = f"{self.class_names.get(cls_id, 'unknown')} {box.conf.item():.2f}"
                cv2.putText(frame, label, (bbox[0], bbox[1]-10), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)
            
            out.write(frame)
            frame_count += 1
        
        cap.release()
        out.release()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –≤–∏–¥–µ–æ –±—ã–ª–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ
        if frame_count == 0:
            raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –Ω–∏ –æ–¥–Ω–æ–≥–æ –∫–∞–¥—Ä–∞")
            
        return output_path
    
    def analyze_detections(self, detections: list, fps: float) -> dict:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∏ –≥—Ä—É–ø–ø–∏—Ä—É–µ—Ç –¥–µ—Ç–µ–∫—Ü–∏–∏"""
        objects = {}
        min_frames = self.MIN_DETECTION_TIME * fps
        
        for frame_idx, frame_dets in enumerate(detections):
            for det in frame_dets:
                cls_id, conf, bbox = det
                
                # –ü–æ–∏—Å–∫ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π —Å –ø—Ä–µ–¥—ã–¥—É—â–∏–º–∏ –æ–±—ä–µ–∫—Ç–∞–º–∏
                matched = False
                for obj_id, obj in objects.items():
                    if self._check_overlap(bbox, obj['last_bbox']):
                        obj['last_bbox'] = bbox
                        obj['frames'].append(frame_idx)
                        matched = True
                        break
                
                if not matched:
                    obj_id = len(objects) + 1
                    objects[obj_id] = {
                        'class': cls_id,
                        'first_frame': frame_idx,
                        'last_bbox': bbox,
                        'frames': [frame_idx]
                    }
        
        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–º—É –≤—Ä–µ–º–µ–Ω–∏
        return {k: v for k, v in objects.items() 
                if len(v['frames']) >= min_frames}
    
    def _check_overlap(self, box1: list, box2: list, threshold: float = 0.5) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ bounding box'–æ–≤"""
        # –†–∞—Å—á–µ—Ç –ø–ª–æ—â–∞–¥–∏ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è
        x_left = max(box1[0], box2[0])
        y_top = max(box1[1], box2[1])
        x_right = min(box1[2], box2[2])
        y_bottom = min(box1[3], box2[3])
        
        if x_right < x_left or y_bottom < y_top:
            return False
            
        intersection = (x_right - x_left) * (y_bottom - y_top)
        area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])
        area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])
        
        return intersection / min(area1, area2) >= threshold
    
    def generate_report(self, objects: dict, fps: float) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã–π –æ—Ç—á–µ—Ç –ø–æ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã–º –æ–±—ä–µ–∫—Ç–∞–º"""
        if not objects:
            return "üîç –û–±—ä–µ–∫—Ç—ã –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã –∏–ª–∏ –≤—Ä–µ–º—è –¥–µ—Ç–µ–∫—Ü–∏–∏ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–æ–µ."
        
        # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ –∫–ª–∞—Å—Å–∞–º
        stats = {}
        for obj_id, obj in objects.items():
            cls_name = self.class_names.get(obj['class'], '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π')
            if cls_name not in stats:
                stats[cls_name] = []
                
            duration = len(obj['frames']) / fps
            start_time = obj['frames'][0] / fps
            end_time = obj['frames'][-1] / fps
            stats[cls_name].append((start_time, end_time, duration))
        
        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç—á–µ—Ç–∞
        report = ["üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–ø–ª–æ–≤–∏–∑–∏–æ–Ω–Ω–æ–≥–æ –≤–∏–¥–µ–æ:"]
        
        for cls_name, items in stats.items():
            report.append(f"\nüîπ {cls_name.capitalize()}: {len(items)} –æ–±—ä–µ–∫—Ç–æ–≤")
            total_time = sum(item[2] for item in items)
            report.append(f"   –û–±—â–µ–µ –≤—Ä–µ–º—è –≤ –∫–∞–¥—Ä–µ: {total_time:.1f} —Å–µ–∫")
            
            for i, (start, end, dur) in enumerate(items, 1):
                report.append(f"   {i}. —Å {start:.1f} –¥–æ {end:.1f} —Å–µ–∫ ({dur:.1f} —Å–µ–∫)")
        
        return "\n".join(report)